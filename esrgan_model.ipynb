{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cab21f8c",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from torchvision.utils import save_image, make_grid\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import glob\n",
    "import random\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92eb762c",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "# Check if CUDA is available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Configuration parameters\n",
    "class Config:\n",
    "    # Model parameters\n",
    "    FEATURE_MAPS = 64\n",
    "    RESIDUAL_BLOCKS = 16\n",
    "    LEAKY_ALPHA = 0.2\n",
    "    DISC_BLOCKS = 4\n",
    "    RESIDUAL_SCALAR = 0.2\n",
    "    \n",
    "    # Training parameters\n",
    "    PRETRAIN_LR = 1e-4\n",
    "    FINETUNE_LR = 3e-5\n",
    "    PRETRAIN_EPOCHS = 100  # Reduced for demonstration\n",
    "    FINETUNE_EPOCHS = 200  # Reduced for demonstration\n",
    "    BATCH_SIZE = 16\n",
    "    SCALE_FACTOR = 4\n",
    "    \n",
    "    # Dataset paths\n",
    "    BASE_DATA_PATH = \"dataset\"\n",
    "    DIV2K_PATH = os.path.join(BASE_DATA_PATH, \"div2k\")\n",
    "    \n",
    "    # Output paths\n",
    "    BASE_OUTPUT_PATH = \"output\"\n",
    "    PRETRAINED_GEN_PATH = os.path.join(BASE_OUTPUT_PATH, \"pretrained_generator.h5\")\n",
    "    GEN_PATH = os.path.join(BASE_OUTPUT_PATH, \"esrgan_generator.h5\")\n",
    "\n",
    "# Create output directory\n",
    "os.makedirs(Config.BASE_OUTPUT_PATH, exist_ok=True)\n",
    "\n",
    "# Dataset preparation\n",
    "class DIV2KDataset(Dataset):\n",
    "    def __init__(self, hr_dir, scale_factor=4, crop_size=128, transform=None):\n",
    "        self.hr_dir = hr_dir\n",
    "        self.scale_factor = scale_factor\n",
    "        self.crop_size = crop_size\n",
    "        self.transform = transform\n",
    "        self.hr_images = sorted(glob.glob(os.path.join(hr_dir, \"*.png\")))\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.hr_images)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        hr_img = Image.open(self.hr_images[idx]).convert('RGB')\n",
    "        \n",
    "        # Random crop\n",
    "        if self.transform:\n",
    "            hr_img = self.transform(hr_img)\n",
    "        \n",
    "        # Create LR image through downsampling\n",
    "        lr_img = F.interpolate(\n",
    "            hr_img.unsqueeze(0), \n",
    "            scale_factor=1.0/self.scale_factor, \n",
    "            mode='bicubic', \n",
    "            align_corners=False\n",
    "        ).squeeze(0)\n",
    "        \n",
    "        return lr_img, hr_img\n",
    "\n",
    "# Data transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.RandomCrop(Config.SCALE_FACTOR * 32),  # Ensure divisibility by scale factor\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomVerticalFlip(),\n",
    "    transforms.ToTensor()\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "835df842",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# ESRGAN Model Architecture\n",
    "class ResidualDenseBlock(nn.Module):\n",
    "    def __init__(self, filters=64, res_scale=0.2):\n",
    "        super(ResidualDenseBlock, self).__init__()\n",
    "        self.res_scale = res_scale\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(filters, filters, 3, padding=1, bias=True)\n",
    "        self.conv2 = nn.Conv2d(filters*2, filters, 3, padding=1, bias=True)\n",
    "        self.conv3 = nn.Conv2d(filters*3, filters, 3, padding=1, bias=True)\n",
    "        self.conv4 = nn.Conv2d(filters*4, filters, 3, padding=1, bias=True)\n",
    "        self.conv5 = nn.Conv2d(filters*5, filters, 3, padding=1, bias=True)\n",
    "        self.lrelu = nn.LeakyReLU(negative_slope=Config.LEAKY_ALPHA, inplace=True)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x1 = self.lrelu(self.conv1(x))\n",
    "        x2 = self.lrelu(self.conv2(torch.cat((x, x1), 1)))\n",
    "        x3 = self.lrelu(self.conv3(torch.cat((x, x1, x2), 1)))\n",
    "        x4 = self.lrelu(self.conv4(torch.cat((x, x1, x2, x3), 1)))\n",
    "        x5 = self.conv5(torch.cat((x, x1, x2, x3, x4), 1))\n",
    "        return x5 * self.res_scale + x\n",
    "\n",
    "class RRDB(nn.Module):\n",
    "    def __init__(self, filters, res_scale=0.2):\n",
    "        super(RRDB, self).__init__()\n",
    "        self.res_scale = res_scale\n",
    "        self.rdb1 = ResidualDenseBlock(filters, res_scale)\n",
    "        self.rdb2 = ResidualDenseBlock(filters, res_scale)\n",
    "        self.rdb3 = ResidualDenseBlock(filters, res_scale)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.rdb1(x)\n",
    "        out = self.rdb2(out)\n",
    "        out = self.rdb3(out)\n",
    "        return out * self.res_scale + x\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, in_channels=3, out_channels=3, filters=64, num_res_blocks=16, upscale_factor=4):\n",
    "        super(Generator, self).__init__()\n",
    "        \n",
    "        # First conv layer\n",
    "        self.conv_first = nn.Conv2d(in_channels, filters, kernel_size=3, stride=1, padding=1)\n",
    "        \n",
    "        # RRDB blocks\n",
    "        rrdb_blocks = []\n",
    "        for _ in range(num_res_blocks):\n",
    "            rrdb_blocks.append(RRDB(filters))\n",
    "        self.rrdb_blocks = nn.Sequential(*rrdb_blocks)\n",
    "        \n",
    "        # Second conv layer after residual blocks\n",
    "        self.conv_after_blocks = nn.Conv2d(filters, filters, kernel_size=3, stride=1, padding=1)\n",
    "        \n",
    "        # Upsampling layers\n",
    "        upsampling = []\n",
    "        for _ in range(2):  # For 4x upscaling (2^2 = 4)\n",
    "            upsampling.extend([\n",
    "                nn.Conv2d(filters, filters * 4, kernel_size=3, stride=1, padding=1),\n",
    "                nn.PixelShuffle(2),\n",
    "                nn.LeakyReLU(Config.LEAKY_ALPHA, inplace=True)\n",
    "            ])\n",
    "        self.upsampling = nn.Sequential(*upsampling)\n",
    "        \n",
    "        # Final output layer\n",
    "        self.conv_last = nn.Conv2d(filters, out_channels, kernel_size=3, stride=1, padding=1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # First conv\n",
    "        feat = self.conv_first(x)\n",
    "        trunk = feat\n",
    "        \n",
    "        # RRDB blocks\n",
    "        trunk = self.rrdb_blocks(trunk)\n",
    "        \n",
    "        # Second conv\n",
    "        trunk = self.conv_after_blocks(trunk)\n",
    "        feat = feat + trunk\n",
    "        \n",
    "        # Upsampling\n",
    "        feat = self.upsampling(feat)\n",
    "        \n",
    "        # Final conv\n",
    "        out = self.conv_last(feat)\n",
    "        return out\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, input_shape=(3, 128, 128), filters=64):\n",
    "        super(Discriminator, self).__init__()\n",
    "        \n",
    "        def discriminator_block(in_filters, out_filters, stride=1, normalize=True):\n",
    "            layers = [nn.Conv2d(in_filters, out_filters, 3, stride, 1)]\n",
    "            if normalize:\n",
    "                layers.append(nn.BatchNorm2d(out_filters))\n",
    "            layers.append(nn.LeakyReLU(Config.LEAKY_ALPHA, inplace=True))\n",
    "            return layers\n",
    "        \n",
    "        self.model = nn.Sequential(\n",
    "            *discriminator_block(input_shape[0], filters, normalize=False),\n",
    "            *discriminator_block(filters, filters, stride=2),\n",
    "            *discriminator_block(filters, filters * 2),\n",
    "            *discriminator_block(filters * 2, filters * 2, stride=2),\n",
    "            *discriminator_block(filters * 2, filters * 4),\n",
    "            *discriminator_block(filters * 4, filters * 4, stride=2),\n",
    "            *discriminator_block(filters * 4, filters * 8),\n",
    "            *discriminator_block(filters * 8, filters * 8, stride=2),\n",
    "            nn.Conv2d(filters * 8, 1, 3, stride=1, padding=1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, img):\n",
    "        return self.model(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0719cde0",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# VGG19 Feature Extractor for Perceptual Loss\n",
    "class VGGFeatureExtractor(nn.Module):\n",
    "    def __init__(self, feature_layer=35, use_bn=False):\n",
    "        super(VGGFeatureExtractor, self).__init__()\n",
    "        from torchvision.models import vgg19\n",
    "        vgg = vgg19(pretrained=True)\n",
    "        self.features = nn.Sequential(*list(vgg.features.children())[:feature_layer]).eval()\n",
    "        for param in self.features.parameters():\n",
    "            param.requires_grad = False\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.features(x)\n",
    "\n",
    "# Loss Functions\n",
    "class ContentLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ContentLoss, self).__init__()\n",
    "        self.l1 = nn.L1Loss()\n",
    "    \n",
    "    def forward(self, sr, hr):\n",
    "        return self.l1(sr, hr)\n",
    "\n",
    "class PerceptualLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(PerceptualLoss, self).__init__()\n",
    "        self.feature_extractor = VGGFeatureExtractor().to(device)\n",
    "        self.l1 = nn.L1Loss()\n",
    "    \n",
    "    def forward(self, sr, hr):\n",
    "        sr_features = self.feature_extractor(sr)\n",
    "        hr_features = self.feature_extractor(hr)\n",
    "        return self.l1(sr_features, hr_features)\n",
    "\n",
    "class AdversarialLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(AdversarialLoss, self).__init__()\n",
    "        self.bce = nn.BCEWithLogitsLoss()\n",
    "    \n",
    "    def forward(self, pred, target):\n",
    "        return self.bce(pred, target)\n",
    "\n",
    "# Training Functions\n",
    "def pretrain_generator(generator, dataloader, epochs, optimizer, criterion, device):\n",
    "    generator.train()\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        epoch_loss = 0\n",
    "        progress_bar = tqdm(dataloader)\n",
    "        \n",
    "        for i, (lr, hr) in enumerate(progress_bar):\n",
    "            lr = lr.to(device)\n",
    "            hr = hr.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            sr = generator(lr)\n",
    "            loss = criterion(sr, hr)\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "            progress_bar.set_description(f\"Epoch {epoch+1}/{epochs} | Loss: {epoch_loss/(i+1):.6f}\")\n",
    "        \n",
    "        # Save sample images\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            with torch.no_grad():\n",
    "                sample_lr = next(iter(dataloader))[0][:4].to(device)\n",
    "                sample_sr = generator(sample_lr)\n",
    "                \n",
    "                # Denormalize\n",
    "                sample_lr = torch.clamp(sample_lr, 0, 1)\n",
    "                sample_sr = torch.clamp(sample_sr, 0, 1)\n",
    "                \n",
    "                # Save grid of images\n",
    "                grid_lr = make_grid(sample_lr, nrow=2, normalize=True)\n",
    "                grid_sr = make_grid(sample_sr, nrow=2, normalize=True)\n",
    "                \n",
    "                save_image(grid_lr, os.path.join(Config.BASE_OUTPUT_PATH, f\"pretrain_lr_epoch_{epoch+1}.png\"))\n",
    "                save_image(grid_sr, os.path.join(Config.BASE_OUTPUT_PATH, f\"pretrain_sr_epoch_{epoch+1}.png\"))\n",
    "    \n",
    "    # Save the pretrained generator\n",
    "    torch.save(generator.state_dict(), Config.PRETRAINED_GEN_PATH)\n",
    "    print(f\"Pretrained generator saved to {Config.PRETRAINED_GEN_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d775671",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def train_esrgan(generator, discriminator, dataloader, epochs, g_optimizer, d_optimizer, \n",
    "                content_criterion, perceptual_criterion, adversarial_criterion, device):\n",
    "    generator.train()\n",
    "    discriminator.train()\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        epoch_g_loss = 0\n",
    "        epoch_d_loss = 0\n",
    "        progress_bar = tqdm(dataloader)\n",
    "        \n",
    "        for i, (lr, hr) in enumerate(progress_bar):\n",
    "            batch_size = lr.size(0)\n",
    "            \n",
    "            # Move data to device\n",
    "            lr = lr.to(device)\n",
    "            hr = hr.to(device)\n",
    "            \n",
    "            # Adversarial ground truths\n",
    "            valid = torch.ones((batch_size, 1, 8, 8), requires_grad=False).to(device)\n",
    "            fake = torch.zeros((batch_size, 1, 8, 8), requires_grad=False).to(device)\n",
    "            \n",
    "            # ------------------\n",
    "            #  Train Generator\n",
    "            # ------------------\n",
    "            g_optimizer.zero_grad()\n",
    "            \n",
    "            # Generate high-resolution images\n",
    "            sr = generator(lr)\n",
    "            \n",
    "            # Adversarial loss\n",
    "            pred_fake = discriminator(sr)\n",
    "            g_adv_loss = adversarial_criterion(pred_fake, valid)\n",
    "            \n",
    "            # Content loss (pixel-wise)\n",
    "            g_content_loss = content_criterion(sr, hr)\n",
    "            \n",
    "            # Perceptual loss\n",
    "            g_percep_loss = perceptual_criterion(sr, hr)\n",
    "            \n",
    "            # Total generator loss\n",
    "            g_loss = 0.01 * g_adv_loss + 1.0 * g_content_loss + 1.0 * g_percep_loss\n",
    "            \n",
    "            g_loss.backward()\n",
    "            g_optimizer.step()\n",
    "            \n",
    "            #  Train Discriminator\n",
    "\n",
    "            d_optimizer.zero_grad()\n",
    "            \n",
    "            # Real loss\n",
    "            pred_real = discriminator(hr)\n",
    "            d_real_loss = adversarial_criterion(pred_real, valid)\n",
    "            \n",
    "            # Fake loss\n",
    "            pred_fake = discriminator(sr.detach())\n",
    "            d_fake_loss = adversarial_criterion(pred_fake, fake)\n",
    "            \n",
    "            # Total discriminator loss\n",
    "            d_loss = (d_real_loss + d_fake_loss) / 2\n",
    "            \n",
    "            d_loss.backward()\n",
    "            d_optimizer.step()\n",
    "            \n",
    "            # Update progress bar\n",
    "            epoch_g_loss += g_loss.item()\n",
    "            epoch_d_loss += d_loss.item()\n",
    "            progress_bar.set_description(\n",
    "                f\"Epoch {epoch+1}/{epochs} | G Loss: {epoch_g_loss/(i+1):.6f} | D Loss: {epoch_d_loss/(i+1):.6f}\"\n",
    "            )\n",
    "        \n",
    "        # Save sample images\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            with torch.no_grad():\n",
    "                sample_lr = next(iter(dataloader))[0][:4].to(device)\n",
    "                sample_sr = generator(sample_lr)\n",
    "                \n",
    "                # Denormalize\n",
    "                sample_lr = torch.clamp(sample_lr, 0, 1)\n",
    "                sample_sr = torch.clamp(sample_sr, 0, 1)\n",
    "                \n",
    "                # Save grid of images\n",
    "                grid_lr = make_grid(sample_lr, nrow=2, normalize=True)\n",
    "                grid_sr = make_grid(sample_sr, nrow=2, normalize=True)\n",
    "                \n",
    "                save_image(grid_lr, os.path.join(Config.BASE_OUTPUT_PATH, f\"train_lr_epoch_{epoch+1}.png\"))\n",
    "                save_image(grid_sr, os.path.join(Config.BASE_OUTPUT_PATH, f\"train_sr_epoch_{epoch+1}.png\"))\n",
    "    \n",
    "    # Save the trained generator\n",
    "    torch.save(generator.state_dict(), Config.GEN_PATH)\n",
    "    print(f\"ESRGAN generator saved to {Config.GEN_PATH}\")\n",
    "\n",
    "# Inference function\n",
    "def inference(generator, lr_img_path, output_path):\n",
    "    generator.eval()\n",
    "    \n",
    "    # Load and preprocess the low-resolution image\n",
    "    lr_img = Image.open(lr_img_path).convert('RGB')\n",
    "    lr_tensor = transforms.ToTensor()(lr_img).unsqueeze(0).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        sr_tensor = generator(lr_tensor)\n",
    "        sr_tensor = torch.clamp(sr_tensor, 0, 1)\n",
    "    \n",
    "    # Convert tensor to image and save\n",
    "    sr_img = transforms.ToPILImage()(sr_tensor.squeeze(0).cpu())\n",
    "    sr_img.save(output_path)\n",
    "    \n",
    "    return sr_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6890ff4d",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    # Create dataset and dataloader\n",
    "    dataset = DIV2KDataset(Config.DIV2K_PATH, scale_factor=Config.SCALE_FACTOR, transform=transform)\n",
    "    dataloader = DataLoader(dataset, batch_size=Config.BATCH_SIZE, shuffle=True, num_workers=4)\n",
    "    \n",
    "    # Initialize models\n",
    "    generator = Generator(\n",
    "        filters=Config.FEATURE_MAPS,\n",
    "        num_res_blocks=Config.RESIDUAL_BLOCKS,\n",
    "        upscale_factor=Config.SCALE_FACTOR\n",
    "    ).to(device)\n",
    "    \n",
    "    discriminator = Discriminator(\n",
    "        filters=Config.FEATURE_MAPS\n",
    "    ).to(device)\n",
    "    \n",
    "    # Initialize loss functions\n",
    "    content_criterion = ContentLoss().to(device)\n",
    "    perceptual_criterion = PerceptualLoss().to(device)\n",
    "    adversarial_criterion = AdversarialLoss().to(device)\n",
    "    \n",
    "    # Pretrain generator\n",
    "    print(\"Pretraining generator...\")\n",
    "    pretrain_optimizer = optim.Adam(generator.parameters(), lr=Config.PRETRAIN_LR)\n",
    "    pretrain_generator(\n",
    "        generator=generator,\n",
    "        dataloader=dataloader,\n",
    "        epochs=Config.PRETRAIN_EPOCHS,\n",
    "        optimizer=pretrain_optimizer,\n",
    "        criterion=content_criterion,\n",
    "        device=device\n",
    "    )\n",
    "    \n",
    "    # Load pretrained generator\n",
    "    generator.load_state_dict(torch.load(Config.PRETRAINED_GEN_PATH))\n",
    "    \n",
    "    # Train full ESRGAN\n",
    "    print(\"Training ESRGAN...\")\n",
    "    g_optimizer = optim.Adam(generator.parameters(), lr=Config.FINETUNE_LR)\n",
    "    d_optimizer = optim.Adam(discriminator.parameters(), lr=Config.FINETUNE_LR)\n",
    "    \n",
    "    train_esrgan(\n",
    "        generator=generator,\n",
    "        discriminator=discriminator,\n",
    "        dataloader=dataloader,\n",
    "        epochs=Config.FINETUNE_EPOCHS,\n",
    "        g_optimizer=g_optimizer,\n",
    "        d_optimizer=d_optimizer,\n",
    "        content_criterion=content_criterion,\n",
    "        perceptual_criterion=perceptual_criterion,\n",
    "        adversarial_criterion=adversarial_criterion,\n",
    "        device=device\n",
    "    )\n",
    "    \n",
    "    # Inference example\n",
    "    print(\"Running inference on a sample image...\")\n",
    "    sample_lr_path = \"sample_lr.png\"  # Replace with your sample image path\n",
    "    sample_sr_path = os.path.join(Config.BASE_OUTPUT_PATH, \"sample_sr.png\")\n",
    "    \n",
    "    if os.path.exists(sample_lr_path):\n",
    "        sr_img = inference(generator, sample_lr_path, sample_sr_path)\n",
    "        \n",
    "        # Display results\n",
    "        lr_img = Image.open(sample_lr_path).convert('RGB')\n",
    "        \n",
    "        plt.figure(figsize=(12, 6))\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.title(\"Low Resolution\")\n",
    "        plt.imshow(lr_img)\n",
    "        plt.axis('off')\n",
    "        \n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.title(\"Super Resolution (ESRGAN)\")\n",
    "        plt.imshow(sr_img)\n",
    "        plt.axis('off')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(f\"Sample image not found at {sample_lr_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
